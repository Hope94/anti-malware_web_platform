import os
os.environ.setdefault ("DJANGO_SETTINGS_MODULE", "pfe_plateforme_web.settings")
import django

django.setup ()
from typing import Optional, Any

import pandas as pd
from sklearn.model_selection import GridSearchCV,train_test_split
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix
import numpy as np
import itertools
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from mlxtend.classifier import StackingClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB,BernoulliNB
from sklearn.linear_model import LogisticRegression,SGDClassifier,PassiveAggressiveClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

from main.models import Model

seed = 7
RANDOM_STATE = 42

N_JOBS = 1
PRE_DISPATCH = '2*n_jobs'

def init_classifier(n_samples):
    clf1 = KNeighborsClassifier(n_neighbors=1)
    clf2 = RandomForestClassifier(random_state=1)
    clf3 = LinearSVC()
    lr = LogisticRegression()

    return {
        'SVC': SVC(),
        'RandomForest': RandomForestClassifier(),
        'LinearSVC': LinearSVC(),
        'NaiveBays': BernoulliNB(),
        'SGD': SGDClassifier(),
        'PassiveAggressive': PassiveAggressiveClassifier(),
        'AdaBoost': AdaBoostClassifier(),
        'GBoosting':GradientBoostingClassifier(),
        'Bagging': BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=seed),
        'Stacking' :  StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr,use_probas=True,
                          average_probas=False)
    }


def init_hyper_params (n_samples):
    svm_parameters = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                      'C': [1, 10, 100, 1000]}
    n_estimators_range  = np.arange(10, 100, 5)
    n_estimators_range_ = np.arange(10, 600, 10)
    randomForest_parameters = {"n_estimators": n_estimators_range,
                               'max_features': ['auto', 'sqrt', 'log2',None],
                               "max_depth": [3, None],
                               "bootstrap": [True, False],
                               "criterion": ["gini", "entropy"]}

    linear_svm_parameters = {'C': np.logspace(-5, 2, num=200)}

    alpha_range =10.0 ** -np.arange(1, 5)
    n =  np.ceil(10**6 /n_samples)

    sgd_parameters = {'loss': ['hinge', 'modified_huber', 'log', 'squared_hinge'],
                      'penalty': ['l1', 'l2', 'elasticnet'],
                      'average': [False, True],
                      'alpha': alpha_range,
                      'learning_rate': ['optimal', 'invscaling'],
                      'n_iter': [n],
                      'eta0': [0.01]
                      }
    adaboost_parameters = {'n_estimators': n_estimators_range_,
                           'learning_rate': [1.0, 0.1],
                           'algorithm': ['SAMME', 'SAMME.R'],
                           'random_state': seed}

    gboost_parameters = {'n_estimators' : n_estimators_range_,
                           'learning_rate':[1.0,0.1,0.01],
                           'max_features': ['auto', 'sqrt', 'log2',None],
                           'loss': ['deviance', 'exponential'],
                           'subsample': [1.0, 0.5],
                           'random_state': seed}

    bagging_parameters = { 'n_estimators':n_estimators_range,
                           'max_samples':1.0,
                           'max_features':1.0,
                           'bootstrap':[True,False],
                           'bootstrap_features':[False,True],
                           'oob_score':False}
    stacking_parameters = {'kneighborsclassifier__n_neighbors': [1, 5],
          'randomforestclassifier__n_estimators': [10, 50],
          'meta-logisticregression__C': [0.1, 10.0]}
    C_values = np.random.uniform(low=1e-10, high=10., size=100)
    passiveaggressive_parameters = {'C': C_values,
                                    'loss': ['hinge', 'squared_hinge'],
                                    'fit_intercept': [True, False],
                                    'random_state': seed}
    return {'SVC': svm_parameters,
                        'RandomForest': randomForest_parameters,
                        'LinearSVC': linear_svm_parameters,
                        'SGD': sgd_parameters,
            'PassiveAggressive':  passiveaggressive_parameters,
            'AdaBoost': adaboost_parameters,
            'GBoosting' : gboost_parameters,
            'Bagging': bagging_parameters,
            'Stacking': stacking_parameters
            }


scores = ['precision', 'recall','roc_auc']

def set_const_n_jobs(n_jobs: int):
    global  N_JOBS
    N_JOBS = n_jobs

def set_const_pre_dispatch(pre_dispatch):
    global PRE_DISPATCH
    PRE_DISPATCH = pre_dispatch




def train_hyper_parameters(estimator,X_train, y_train, nb_splits, param_grid,scorer):
    # use cross-validation to evaluate the performance of each parameter combination : grid search with cross-validation.
    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=nb_splits,
                               scoring= scorer,
                               n_jobs=1, pre_dispatch=PRE_DISPATCH
                                ,return_train_score=True)
    clf.fit(X_train, y_train)
    return {'best_params': clf.best_params_,
            'best_score': clf.best_score_,
            'best_estimator': clf.best_estimator_,
            'cv_results': pd.DataFrame(clf.cv_results_)}



def evaluate(estimator, X_test, y_test):
    try:
        auc = roc_auc_score(y_test, estimator.decision_function(X_test))
    except :
        auc = roc_auc_score(y_test,estimator.predict_proba(X_test)[:,1])
    conf_matrix = confusion_matrix(y_test, estimator.predict(X_test), labels=[0, 1])
    tn = conf_matrix[0, 0]
    tp = conf_matrix[1, 1]
    fp = conf_matrix[0, 1]
    fn = conf_matrix[1, 0]
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    recall = tp / (tp + fn)
    precision = tp / (tp + fp)
    fpr = fp / (fp + tn)
    f1 = 2 * (precision * recall) / (precision + recall)
    return {
        'positives': tp + fn,
        'negatives': tn + fp,
        'auc': auc,
        'accuracy': accuracy,
        'recall': recall,
        'precision': precision,
        'f1': f1,
        'tp': tp,
        'tn': tn,
        'fp': fp,
        'fn': fn,
        'tpr': recall,
        'fpr': fpr
    }

import _pickle as pk
def save_object(obj, filename):
    with open(filename, 'wb') as output:  # Overwrites any existing file.
        pk.dump(obj, output)
def load_object(filename):
    with open(filename, 'rb') as output:
        return pk.load(output)

def MDI(estimator,vc,cv,nb_features):
    importances = estimator.feature_importances_
    std = np.std([tree.feature_importances_ for tree in estimator.estimators_],
                 axis=0)
    indices= np.argsort(importances)[::-1]

    selected_feature = []
    rank = []
    features = []
    scores = []
    for f in range(nb_features):
        rank.append(f+1)
        features.append(cv[indices[f]])
        scores.append(importances[indices[f]])
        if importances[indices[f]] > 0:
            selected_feature.append(cv[indices[f]])
    return selected_feature



from main.vector_space_constractor.vectorizer import *
import time
if __name__ == '__main__':
    start=time.time()
    m=Model.objects.get(type_cls='RandomForest')
    estimator=load_object(m.path)
    vc,cv=build_vector_space_dict(m.vs.features.all())
    features=MDI(estimator,vc,cv,100)
    #vs=insert_vector_space(rep_type=,scal=)

    print(features)
    print('time:',time.time()-start)

